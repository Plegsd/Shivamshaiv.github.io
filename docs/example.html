

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Research Ideas &mdash; LambdaZero v0.0.1 documentation</title>
  

  
  <link rel="stylesheet" href="static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="static/documentation_options.js"></script>
        <script src="static/jquery.js"></script>
        <script src="static/underscore.js"></script>
        <script src="static/doctools.js"></script>
        <script src="static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="LambdaZero.chem package" href="generated/LambdaZero.chem.html" />
    <link rel="prev" title="Welcome to LambdaZero documentation" href="index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> LambdaZero
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Get Started</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Research Ideas</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#john">John</a></li>
<li class="toctree-l2"><a class="reference internal" href="#emmanuel">Emmanuel</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#persistent-tree-search">Persistent Tree Search</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#results">Results</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#persistent-mcts">Persistent MCTS</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#maksym">Maksym</a></li>
<li class="toctree-l2"><a class="reference internal" href="#priyesh">Priyesh</a></li>
<li class="toctree-l2"><a class="reference internal" href="#moksh">Moksh</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#exploration-experiments">Exploration Experiments</a></li>
<li class="toctree-l3"><a class="reference internal" href="#environment-parameters">Environment Parameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="#alphazero">AlphaZero</a></li>
<li class="toctree-l3"><a class="reference internal" href="#lambdabo-bayesopt-rl">LambdaBO (BayesOpt + RL)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id1">Results</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#uncertainty-learning">Uncertainty Learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#algorithm-regression-to-be-updated">Algorithm (Regression): <em>To be updated</em></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#sumana">Sumana</a></li>
<li class="toctree-l2"><a class="reference internal" href="#howard">Howard</a></li>
<li class="toctree-l2"><a class="reference internal" href="#victor">Victor</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#reward-predictor">Reward Predictor</a></li>
<li class="toctree-l3"><a class="reference internal" href="#backpropagated-loss">Backpropagated Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#non-uniform-sampling-of-molecules-for-training">Non-uniform sampling of molecules for training</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id2">Results</a></li>
<li class="toctree-l3"><a class="reference internal" href="#training-without-low-energy-tail">Training without low energy tail</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#joanna">Joanna</a></li>
<li class="toctree-l2"><a class="reference internal" href="#samin">Samin</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#graph-bottleneck">Graph_bottleneck</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cross-entropy-method">Cross entropy method</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#summary-of-meetings">Summary of meetings</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#august-26">August 26</a></li>
<li class="toctree-l3"><a class="reference internal" href="#august-19-8-30">August 19 8:30</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="generated/LambdaZero.chem.html">LambdaZero.chem package</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/LambdaZero.environments.html">LambdaZero.environments package</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/LambdaZero.examples.html">LambdaZero.examples package</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/LambdaZero.datasets.html">LambdaZero.datasets package</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/LambdaZero.inputs.html">LambdaZero.inputs package</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/LambdaZero.models.html">LambdaZero.models package</a></li>
<li class="toctree-l1"><a class="reference internal" href="generated/LambdaZero.utils.html">LambdaZero.utils package</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">LambdaZero</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Research Ideas</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/example.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="research-ideas">
<h1>Research Ideas<a class="headerlink" href="#research-ideas" title="Permalink to this headline">¶</a></h1>
<p>Here is a brief enumeration of research directions and results</p>
<div class="section" id="john">
<h2>John<a class="headerlink" href="#john" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://www.notion.so/LambdaBO-6e78c9ec399844a6a05dc3491a3c6394">LambdaBO</a></p>
</div>
<div class="section" id="emmanuel">
<h2>Emmanuel<a class="headerlink" href="#emmanuel" title="Permalink to this headline">¶</a></h2>
<div class="section" id="persistent-tree-search">
<h3>Persistent Tree Search<a class="headerlink" href="#persistent-tree-search" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://github.com/MKorablyov/LambdaZero/blob/persistent_search_new_reward/LambdaZero/environments/persistent_search/persistent_tree.py">Driver
code</a>
| <a class="reference external" href="https://github.com/MKorablyov/LambdaZero/blob/persistent_search_new_reward/LambdaZero/examples/persistent_search/train_persistent_tree.py">Training
Code</a></p>
<p><strong>Overview</strong>: an always expanding search where nodes are sampled
according to some priority, to be explored by an RL agent. The RL agent
is only allowed to add molecule blocks to a given molecule, making this
search a DAG (in practice a tree).</p>
<dl class="simple">
<dt><strong>Algorithm</strong>: </dt><dd><ul class="simple">
<li><p>Initialize tree with available blocks (optional, seed tree with topK molecules seen in previous runs) </p></li>
<li><p>S = sample k nodes from tree </p></li>
<li><dl class="simple">
<dt>Repeat: </dt><dd><ul>
<li><p>sample pi(a|s), take step for each a,s -&gt; s? </p></li>
<li><p>Train agent to predict reward and value (see below) </p></li>
<li><p>Insert all s? into tree </p></li>
<li><p>For all (s,a,s?), if s? is terminal, sample new node </p></li>
<li><p>S = [S? | new samples] </p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Every n iterations: </dt><dd><ul>
<li><dl class="simple">
<dt>Prune tree if it is larger than <code class="docutils literal notranslate"><span class="pre">prune_at</span></code> </dt><dd><ul>
<li><p>Find 25th percentile of rewards -&gt; thresh </p></li>
<li><p>Compute max(reward in subtree(node)) for all nodes </p></li>
<li><p>Prune all nodes who?s max subtree reward is &lt;= thresh </p></li>
</ul>
</dd>
</dl>
</li>
<li><p>Update for all nodes the value target for the RL actor, either: </p></li>
<li><p>the max subtree/descendent reward </p></li>
<li><p>the averaged montecarlo return of the subtree </p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>In parallel </dt><dd><ul>
<li><p>Compute PredDock reward for top nodes </p></li>
<li><p>Compute SimDock reward for top nodes</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
<ul class="simple">
<li><p><strong>Actor</strong>: currently a GNN on the atom graph, outputs Q(s,a) where Q is
trained to predict either the montecarlo return or the maximum reward of
any descendent. The policy pi(a|s) is a boltzmann policy with some
temperature.</p></li>
<li><p><strong>Sampling</strong>: nodes are sampling with probability p_i /
sum_j p_j, i.e. their priority over the sum of all priorities.
Priority is currently computed as the value of the node (max subtree
reward or montecarlo return).</p></li>
<li><p><strong>Reward</strong>: The reward of a node is defined as the most precise reward computed so far. There are 3 levels:</p></li>
</ul>
<p>1. A forward pass of the actor predicts the molecule?s reward (?free?
when node is expanded, 1ms/mol) 1. PredDockReward, which predicts Dock
sim with a pretrained MPNN (5ms/mol) 1. Dock sim reward, which predicts
the docking energy with some calculation (15s/mol)</p>
<div class="section" id="results">
<h4>Results<a class="headerlink" href="#results" title="Permalink to this headline">¶</a></h4>
<p>Loglinear reward hypothesis seems to hold (red is random search):
<img alt="image0" src="https://i.imgur.com/IINBYAz.png" /></p>
<p>in-RAM tree size does seem to matter, but perhaps not in the long run.
Interestingly a smaller buffer seems to be better early on. It may hurt
after 10M molecules but this takes a long time to reach (dotted lines
are top-1): <img alt="image1" src="https://i.imgur.com/2RmgeBg.png" /></p>
<p>This algorithm also learns a nice pareto front of binding energy to
“discount” (i.e. synthesizability * qed), color is time (red molecules
are the most recently found, blue the oldest): <img alt="image2" src="https://i.imgur.com/ZyMDLKR.png" /></p>
<p>TODO: ^ for boltzmann baseline</p>
<p>As time goes, this also finds molecules with higher energy but lower
discount, although this seems to converge (red is binned average):
<img alt="image3" src="https://i.imgur.com/7WeXxsz.png" /></p>
</div>
</div>
<div class="section" id="persistent-mcts">
<h3>Persistent MCTS<a class="headerlink" href="#persistent-mcts" title="Permalink to this headline">¶</a></h3>
<p>I’m currently working on a persistent MCTS, I’m not yet sure how to best
parallelize it and handle the persitence part (which induces staleness
which presumably needs to be updated periodically).</p>
<p>Do we need to update priors? Check KL divergence</p>
</div>
</div>
<hr class="docutils" />
<div class="section" id="maksym">
<h2>Maksym<a class="headerlink" href="#maksym" title="Permalink to this headline">¶</a></h2>
</div>
<hr class="docutils" />
<div class="section" id="priyesh">
<h2>Priyesh<a class="headerlink" href="#priyesh" title="Permalink to this headline">¶</a></h2>
<p>Walk Convolutions - <a class="reference external" href="https://bit.ly/3hzgqLA">Project Plan</a> Project
Details - <a class="reference external" href="https://bit.ly/2ZDwCW8">PPT</a></p>
</div>
<hr class="docutils" />
<div class="section" id="moksh">
<h2>Moksh<a class="headerlink" href="#moksh" title="Permalink to this headline">¶</a></h2>
<p><em>Note: The current reward uses a MPNN trained to predict the docking
energy. However it is trained with only molecules with max energy of
about 3.2. So the reward (which is the dockscore * (Synthesizability
discount * QED discount) ) out at :math:`sim2.9`, so most of the
methods max out around that value. This was also confirmed by running
Emmanuel’s Persistent Search with and without a full docking simulation.
The runs without docking simulation maxed out at around 3 whereas it
reaches maximum rewards of around 4 with docking simulations (Emmanuel’s
plot).</em></p>
<div class="figure align-default">
<img alt="" src="https://i.imgur.com/QtkllxD.png" />
</div>
<div class="section" id="exploration-experiments">
<h3>Exploration Experiments<a class="headerlink" href="#exploration-experiments" title="Permalink to this headline">¶</a></h3>
<ol class="arabic simple">
<li><p><strong>Random Network Distillation</strong></p></li>
</ol>
<p><a class="reference external" href="https://github.com/MKorablyov/LambdaZero/blob/rnd_support/LambdaZero/models/ppo_rnd.py">Relevant file in
code</a>
| <a class="reference external" href="https://github.com/MKorablyov/LambdaZero/tree/rnd_support/LambdaZero/examples/PPO_RND">Experiment
Folder</a></p>
<p><a class="reference external" href="https://arxiv.org/abs/1810.12894">Random Network Distillation (RND)</a>
is a widely used exploration method. RND proposes adding an intrinsic
reward to encourage exploration of novel states.</p>
<p>This intrinsic reward is computed with two randomly initialized
networks: predictor network <span class="math notranslate nohighlight">\(f_\phi\)</span> and target network
<span class="math notranslate nohighlight">\(f_{\phi*}\)</span>, which map the state to a <span class="math notranslate nohighlight">\(d\)</span>-dimensional
embedding.</p>
<p>where <span class="math notranslate nohighlight">\(x_{t+1}\)</span> is the normalized next state, and the reward
<span class="math notranslate nohighlight">\(r_i^t\)</span> is normalized by maintaining a running mean and standard
deviation. The weights for the target network, <span class="math notranslate nohighlight">\(\phi*\)</span> are fixed,
whereas the weights of the predictor <span class="math notranslate nohighlight">\(\phi\)</span> are learned.
Essentially, the predictor tries to match the target networks (random)
output. So this reward will lower when the state is already visited.
This can be viewed as the quantification of uncertainty in prediction a
constant zero function. For learning with this intrinsic reward, we can
decompose the value function as <span class="math notranslate nohighlight">\(V = V_i + \alpha V_e\)</span>, where
<span class="math notranslate nohighlight">\(V_i\)</span> and <span class="math notranslate nohighlight">\(V_e\)</span> represent the intrinsic and extrinsic value,
which can be two heads of a shared larger network and be learned with
different discount factors (<span class="math notranslate nohighlight">\(\gamma_i\)</span> and <span class="math notranslate nohighlight">\(\gamma_e\)</span>).
<span class="math notranslate nohighlight">\(\alpha\)</span> is a hyperparameter used for assigning different
weightage to the intrinsic reward.</p>
<p><strong>Results</strong> We incorporate this method with <a class="reference external" href="https://arxiv.org/abs/1707.06347">Proximal Policy
Optimization</a> and test it on the
<code class="docutils literal notranslate"><span class="pre">Block_Mol_Graph_v1</span></code> environment in LambdaZero. We perform experiments
with different values of <span class="math notranslate nohighlight">\(\alpha\)</span>. The embedding dimension for the
RND networks is fixed to 64.</p>
<p><img alt="image4" src="https://i.imgur.com/aRuoeVD.png" /> <img alt="image5" src="https://i.imgur.com/hM3ivvP.png" /></p>
<p>Results for the experiments with RND. The x-axis represents number of
steps (i.e. number of molecules, and the y-axis represents the
normalized reward). The green plot on the top is the PPO baseline. The
performance of RND with all values of <span class="math notranslate nohighlight">\(\alpha\)</span> is worse than the
PPO baselines.</p>
<p>The performance of RND is significantly worse than the PPO baseline, in
terms of both, the mean as well max rewards. The mean reward is also
much smaller, indicating that the agent is unable to exploit the good
states that it encounters. This is also supported by looking at the
entropy of the policy. The entropy is high indicating the agent does not
reach a point where it starts to exploit, rather than keep exploring.</p>
<ol class="arabic simple" start="2">
<li><p><strong>Restart Buffer</strong></p></li>
</ol>
<p><a class="reference external" href="https://github.com/MKorablyov/LambdaZero/blob/rnd_support/LambdaZero/environments/persistent_search/persistent_buffer.py">Relevant File in
Code</a>
|
<a class="reference external" href="https://github.com/MKorablyov/LambdaZero/tree/rnd_support/LambdaZero/examples/PPO_RND/config.py">Experiments</a></p>
<p>We can also encourage exploration in the molecule space by starting new
episodes from previously encountered molecules. More specifically, a
molecule <span class="math notranslate nohighlight">\(x_t\)</span> obtained at the end of an episode is added to a
buffer <span class="math notranslate nohighlight">\(B\)</span>, if it’s molecular fingerprint similarity is less than
a threshold <span class="math notranslate nohighlight">\(\tau \in [0, 1]\)</span>, where <span class="math notranslate nohighlight">\(\tau = 0.6\)</span> indicates
that two molecules are chemically different. And at the beginning of
each episode, we sample a molecule from the buffer as the starting
molecule. In principle this should encourage the agent to explore the
regions near molecules already discovered, which could be promising. The
size of the buffer <span class="math notranslate nohighlight">\(|B|\)</span> is fixed, and on reaching that size, the
molecules with lower rewards are pruned.</p>
<p><strong>Results</strong> We perform experiments with various values of <span class="math notranslate nohighlight">\(\tau\)</span>
(0.5, 0.6, 0.7), and size of buffer fixed at 500,000. The experiments
are again performed with PPO, on a modified with the buffer. <span class="math notranslate nohighlight">\(p\)</span>
is the probability of starting an episode from a random molecule instead
of a molecule from the buffer. <span class="math notranslate nohighlight">\(pr\)</span> is the probabilty of adding a
molecule to the buffer, as an ablation for checking if the similarity
metric is useful (no threshold used in this case).</p>
<!-- ![](https://i.imgur.com/p5rQ4LT.png)
![](https://i.imgur.com/0hf21PN.png)
 --><div class="figure align-default">
<img alt="" src="https://i.imgur.com/bMdWoxW.png" />
</div>
<p>Results for the experiments with the restart buffer. The x-axis
represents number of steps (i.e. number of molecules, and the y-axis
represents the normalized reward).</p>
<p>The buffer acts as an explicit constraint on the diversity of the
molecule regions that the agent explores. This results in poor
performance, since there is no mechanism to balance the exploration with
exploitation. So the agent keeps exploring without ever trying to
exploit the promising regions. Thus the performance suffers
considerably.</p>
<div class="figure align-default">
<img alt="" src="https://i.imgur.com/8yFyguZ.png" />
</div>
<p><img alt="image6" src="https://i.imgur.com/BCcJeYc.png" /> <img alt="image7" src="https://i.imgur.com/Yls1Ucf.png" /></p>
<p><em>Randomly add to buffer</em> <img alt="image8" src="https://i.imgur.com/GKQZDE5.png" /></p>
<p><em>Higher Thresholds</em> <img alt="image9" src="https://i.imgur.com/YDwhOhF.png" /></p>
<p>Relevant Papers: <a class="reference external" href="https://arxiv.org/pdf/1811.11298.pdf">https://arxiv.org/pdf/1811.11298.pdf</a>
<a class="reference external" href="https://arxiv.org/pdf/1703.02660.pdf">https://arxiv.org/pdf/1703.02660.pdf</a></p>
<p><strong>Things to try out</strong> * Higher threshold (allowing all mols to the
buffer) [Done] * Start episodes with molecule from buffer or random
molecule with 0.5 probability for each [Done] * Use the value instead
of reward to sort the buffer * Add molecules to buffer randomly with
some probability p [done]</p>
<ol class="arabic simple" start="3">
<li><p><strong>Entropy Regularization</strong></p></li>
</ol>
<p><a class="reference external" href="https://github.com/MKorablyov/LambdaZero/blob/rnd_support/LambdaZero/examples/PPO/config.py">Experiments</a></p>
<p>Entropy Regularization has been shown to improve the optimization
landscape in Deep Reinforcement Learning. In settings with sparse
rewards, entropy regularization can help with exploration by encouraging
the agent to select different actions in similar situations. The entropy
of the current policy distribution is added as a regularizing term to
the loss.</p>
<p>We assign an entropy coefficient which controls the contribution of this
regularizing term to the overall loss function. This coefficient can
also be varied with time according to a schedule.</p>
<p><strong>Results</strong> We perform experiments with different values and schedules
for the entropy coefficient(<span class="math notranslate nohighlight">\(\beta\)</span>), with PPO as the base
algorithm on the <code class="docutils literal notranslate"><span class="pre">Block_Mol_Graph_v1</span></code> environment.</p>
<div class="figure align-default">
<img alt="" src="https://i.imgur.com/6GvQBbC.png" />
</div>
<div class="figure align-default">
<img alt="" src="https://i.imgur.com/64DPSDD.png" />
</div>
<p>Results for the experiments with entropy regularization. The x-axis
represents number of steps (i.e. number of molecules, and the y-axis
represents the normalized reward).</p>
<p>Entropy regularization, given the right coefficient, significantly
outperform the PPO baseline, in terms of both the best as well as
average performance. When set too high the agent is again, unable to
exploit promising regions. And if set too low, there will not be any
significant improvement over the baselines.</p>
</div>
<div class="section" id="environment-parameters">
<h3>Environment Parameters<a class="headerlink" href="#environment-parameters" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://github.com/MKorablyov/LambdaZero/blob/rnd_support/LambdaZero/examples/PPO/config.py">Experiments</a></p>
<p>Another important aspect that affects the performance is the environment
parameters. For instance, if <code class="docutils literal notranslate"><span class="pre">num_steps</span></code>, which controls the maximum
number of steps in an episode, is set too low then the episode
trajectories will be shorter and the agent not be able to explore enough
in that particular region. We perform some experiments to determine the
parameters which perform well in general with PPO, assuming they would
generalize to other algorithms as well.</p>
<p>The parameters tuned are: <code class="docutils literal notranslate"><span class="pre">num_steps</span></code>: maximum number of steps in an
episode, <code class="docutils literal notranslate"><span class="pre">max_block</span></code>: maximum number of blocks allowed on a candidate
molecule, <code class="docutils literal notranslate"><span class="pre">random_blocks</span></code>: number of random blocks at the start of an
episode.</p>
<div class="figure align-default">
<img alt="" src="https://i.imgur.com/OgAbL9D.png" />
</div>
<div class="figure align-default">
<img alt="" src="https://i.imgur.com/Mu06RXZ.png" />
</div>
<p>Results for the experiments with the environment parameters. The x-axis
represents number of steps (i.e. number of molecules, and the y-axis
represents the normalized reward).</p>
<p>Increasing <code class="docutils literal notranslate"><span class="pre">random_blocks</span></code> in principle should force the agent to
explore the block removal action more, since the initial randomly
assembled molecule will most likely not be good. However, this would
work only when the agent is allowed more steps in the episode. So
increasing both the number of steps and and random blocks (along with
max blocks) seems to perform well in terms of the top molecules
encountered. However, the mean performance suffers considerably since,
when starting with 4 random blocks the agent is unlikely to reach a good
molecule often. However, just increasing the number of steps and number
of blocks helps considerably and outperforms all other modifications.</p>
</div>
<div class="section" id="alphazero">
<h3>AlphaZero<a class="headerlink" href="#alphazero" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://github.com/MKorablyov/LambdaZero/tree/rnd_support/LambdaZero/examples/AlphaZero">Experiments</a>
<a class="reference external" href="https://science.sciencemag.org/content/362/6419/1140">AlphaZero</a>
introduced a generalized policy iteration algorithm for learning to play
games such as chess using self-play. It uses a variant of the UCT
algorithm, <em>pUCT</em> with a value function and <em>prior</em> policy (not in the
traditional bayesian context) represented by neural nets. The action is
picked as follows.</p>
<p>where <span class="math notranslate nohighlight">\(\pi_\theta\)</span> is the policy prior and <span class="math notranslate nohighlight">\(Q\)</span> is
action-value function, and <span class="math notranslate nohighlight">\(n(x,a)\)</span> is the visit count of action
<span class="math notranslate nohighlight">\(a\)</span> at state <span class="math notranslate nohighlight">\(x\)</span> in the MCTS. To use AlphaZero in the
LambdaZero setting, we use <a class="reference external" href="https://arxiv.org/abs/1807.01672">ranked
rewards</a>.</p>
<p>It can be shown that AlphaZero approximates the solution of a
regularized policy optimization problem. Using this insight, <a class="reference external" href="https://arxiv.org/abs/2007.12509">Monte
Carlo Tree Search as Regularized Policy
Optimization</a> proposes using exact
solution of this regularized policy optimization problem. So instead of
using the empirical visit distribution (<span class="math notranslate nohighlight">\(\hat{\pi}\)</span>), we use the
solution of the policy optimization (<span class="math notranslate nohighlight">\(\bar{\pi}\)</span>) instead.</p>
<p>where <span class="math notranslate nohighlight">\(S\)</span> is an <span class="math notranslate nohighlight">\(|A|\)</span>-dimensional simplex, and <span class="math notranslate nohighlight">\(KL\)</span> is
the KL divergence. This can also be reformulated as follows:</p>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> is selected such that <span class="math notranslate nohighlight">\(\bar{\pi}\)</span> is a valid
probability distribution. We act and search according to
<span class="math notranslate nohighlight">\(\bar{\pi}\)</span> instead of <span class="math notranslate nohighlight">\(\hat{\pi}\)</span> and Equation (4)
respectively. The prior policy is updated with <span class="math notranslate nohighlight">\(\bar{\pi}\)</span>. Since
<span class="math notranslate nohighlight">\(\bar{\pi}\)</span> depends on the Q values instead of the number of
visits, the policy can learn promising new actions without requiring a
large number of simulations. Thus, this formulation would help
considerably in scenarios where the simulation budget is low.</p>
<p><strong>Results</strong> We compare AlphaZero with the proposed policy optimization
based modification(AlphaZero-PO) as well as a PPO baseline. We perform
experiments with different simulation budgets (100, 200, 800).</p>
<p><img alt="image10" src="https://i.imgur.com/YKaocj1.png" /> <img alt="image11" src="https://i.imgur.com/3ED5dBK.png" /> Results for the experiments with AlphaZero, with
number of simulations as 800. The x-axis represents number of steps
(i.e. number of molecules, and the y-axis represents the normalized
reward).</p>
<p><img alt="image12" src="https://i.imgur.com/NAqE1wN.png" /> <img alt="image13" src="https://i.imgur.com/YQJYDFG.png" /> Results for the experiments with AlphaZero, with
number of simulations as 200. The x-axis represents number of steps
(i.e. number of molecules, and the y-axis represents the normalized
reward).</p>
<p><img alt="image14" src="https://i.imgur.com/oO1J1cm.png" /> <img alt="image15" src="https://i.imgur.com/40w1rS6.png" /> Results for the experiments with AlphaZero, with
number of simulations as 100. The x-axis represents number of steps
(i.e. number of molecules, and the y-axis represents the normalized
reward).</p>
<p><img alt="image16" src="https://i.imgur.com/MrCAIOU.png" /> <img alt="image17" src="https://i.imgur.com/wiJ8l8z.png" /></p>
<p><img alt="image18" src="https://i.imgur.com/CM2OQh2.png" /> <img alt="image19" src="https://i.imgur.com/oSTIahh.png" /></p>
<p>AlphaZero consistently outperforms PPO by a significant margin, even
when number of simulations is set to 100. The policy optimization based
modification also seems to improve the performance over AlphaZero. It
reaches the maximum reward much faster than the AlphaZero baseline.
However since the rewards max out, it is hard to quantify to what
extent.</p>
<p><img alt="image20" src="https://i.imgur.com/MYCpWNq.png" /> <img alt="image21" src="https://i.imgur.com/AgZ4X0I.png" /></p>
<p><strong>Things to try out</strong> * Lower learning rates. [Done] * Lower number of
simulations [Done]</p>
</div>
<div class="section" id="lambdabo-bayesopt-rl">
<h3>LambdaBO (BayesOpt + RL)<a class="headerlink" href="#lambdabo-bayesopt-rl" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://github.com/MKorablyov/LambdaZero/blob/bayesian_reward/LambdaZero/environments/reward.py#L387">Code</a>
|
<a class="reference external" href="https://github.com/MKorablyov/LambdaZero/tree/bayesian_reward/LambdaZero/examples/bayesian_models/rl">Experiments</a>
One of the main aspects of LambdaZero is to learn to leverage multiple
oracles with different fidelities and computational cost to guide search
through the space of molecules. Bayesian Optimization literature has lot
of work in this problem setting. Most of these methods however do not
scale well to higher dimensional problems. The idea (proposed in John
and Miguel’s writeup) is to use reinforcement learning as the inner loop
for collecting samples for the bayesian optimization. The RL agent will
in turn recieve a signal from the bayesian model as it’s reward.</p>
<p>In the reinforcement learning setup, we have an MDP where the state is
the current molecule and the actions are . The reward <span class="math notranslate nohighlight">\(R\)</span> is
defined as the value of a bayesian acquisition function defined on the
model with uncertainty (say UCB (<span class="math notranslate nohighlight">\(R = \mu + \beta * \sigma\)</span>) where
<span class="math notranslate nohighlight">\(\mu\)</span> is the mean and <span class="math notranslate nohighlight">\(\sigma\)</span> is the deviation of the
reward prediction). Whenever the RL agent suggests a molecule to
evaluate at the end of the episode, it is added to a buffer.</p>
<p>Every <span class="math notranslate nohighlight">\(n\)</span> molecules collected in the buffer, we acquire the batch
with the highest acquistion value and retrain the model with uncertainty
with this acquired batch. The RL agent then uses this updated model for
the reward.</p>
<div class="figure align-default">
<img alt="" src="https://i.imgur.com/fLQk8cB.png" />
</div>
<div class="section" id="id1">
<h4>Results<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h4>
<div class="figure align-default">
<img alt="" src="https://i.imgur.com/FkCASGc.png" />
</div>
</div>
</div>
<div class="section" id="uncertainty-learning">
<h3>Uncertainty Learning<a class="headerlink" href="#uncertainty-learning" title="Permalink to this headline">¶</a></h3>
<p>Link to notebook:
<a class="reference external" href="https://colab.research.google.com/drive/1M2_Tq6m09ySHCffYshj1GLW7O24s6HxD?usp=sharing">https://colab.research.google.com/drive/1M2_Tq6m09ySHCffYshj1GLW7O24s6HxD?usp=sharing</a></p>
<div class="section" id="algorithm-regression-to-be-updated">
<h4>Algorithm (Regression): <em>To be updated</em><a class="headerlink" href="#algorithm-regression-to-be-updated" title="Permalink to this headline">¶</a></h4>
<p>Pretrain a Kernel Density Estimator <span class="math notranslate nohighlight">\(d\)</span> on training data
<span class="math notranslate nohighlight">\(x\sim D\)</span>. <span class="math notranslate nohighlight">\(L_a\)</span> = Initialize() <span class="math notranslate nohighlight">\(L_f\)</span> = Initialize()
<span class="math notranslate nohighlight">\(L_e\)</span> = Initialize() Initialize <span class="math notranslate nohighlight">\(D_{ood}\)</span> with OOD samples
(uniformly sampled) t = 0 <strong>repeat</strong> for batches (x, y) from <span class="math notranslate nohighlight">\(D\)</span>:
* sample <span class="math notranslate nohighlight">\((x_{ood}, y_{ood}) \sim D_{ood}\)</span> * <strong>if</strong> t mod N == 0
then: * y2 ~ P(Y |x) * data = ((x, y),(x, y2))) * errors = (([x,
d(x)], e(f(y), y)),([x, d(x)], e(f(y), y2)), ([<span class="math notranslate nohighlight">\(x_{ood}\)</span>,
d(<span class="math notranslate nohighlight">\(x_{ood}\)</span>)], e(f(<span class="math notranslate nohighlight">\(x_{ood}\)</span>), <span class="math notranslate nohighlight">\(y_{ood}\)</span>))) *
Update(<span class="math notranslate nohighlight">\(L_a\)</span>,(x,(y-y2)^2 / 2)) * <strong>else</strong> * errors = (([x,
d(x)], e(f(x), y)), ([<span class="math notranslate nohighlight">\(x_{ood}\)</span>, d(<span class="math notranslate nohighlight">\(x_{ood}\)</span>)],
e(f(<span class="math notranslate nohighlight">\(x_{ood}\)</span>), <span class="math notranslate nohighlight">\(y_{ood}\)</span>))) * data = ((x, y)) * <strong>end
if</strong> * Update(<span class="math notranslate nohighlight">\(L_f\)</span>, data) * Update(<span class="math notranslate nohighlight">\(L_u\)</span>, errors) *
retrain <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(e\)</span> on all data seen so far * t +=1</p>
<p>YB: it would be good to show the pseudo-code with all the bells and
whistles (i.e. the extension with OOD data for <span class="math notranslate nohighlight">\(\hat{u}\)</span> and
density estimator).</p>
<p>Extensions: 1. Train <span class="math notranslate nohighlight">\(\hat{u}\)</span> with additional information, for
instance the density estimated by a density estimation model pretrained
on the training data.</p>
<p>YB: the density estimate seems too smooth (you should make sure to
cross-validate the kernel width).</p>
<ol class="arabic simple" start="3">
<li><p>Uniformly sample OOD points and use these to train <span class="math notranslate nohighlight">\(\hat{u}\)</span> as
well. How are the OOD points used for training? In each iteration, a
batch of OOD samples is added to the training batch for
<span class="math notranslate nohighlight">\(\hat{u}\)</span>.</p></li>
</ol>
<!-- YB: yes, althought it would be interesting to see how many OOD examples are needed for this to work well. Hopefully a small fraction (like 10% or 20% of each batch as OOD examples is sufficient, although the answer may depend on the problem and dimensionality).

The training points are from the function $\sin 2 \pi x$ with gaussian noise($\mu=0$, $\sigma=0.1$).

**Without OOD Samples**:
![](https://i.imgur.com/QcYhunN.png)
![](https://i.imgur.com/buLde2I.png)


After tuning KDE:
![](https://i.imgur.com/64dQsGM.png)

![](https://i.imgur.com/VJVztRz.png)

![](https://i.imgur.com/NHQru2m.png)

![](https://i.imgur.com/k6iKrWA.png)

![](https://i.imgur.com/jzN5lI2.png)


Setting Frequency N=3:

![](https://i.imgur.com/lmgNead.png)

![](https://i.imgur.com/clCaUdz.png)

<!-- ![](https://i.imgur.com/qSCkSkG.png) -->

<!-- **With OOD Samples**
![](https://i.imgur.com/4Clfyhs.png)

YB: The u_loss curve seems to have some trouble initially. Maybe learning rate too large initially. It looks like u still has some way to go before converging. -->

<!-- ![](https://i.imgur.com/JfD2bGg.png) -->
<!-- ![](https://i.imgur.com/ZhVcNBj.png) -->


<!-- YB: Nice but still underestimates uncertainty in several places. What fraction of minibatches of $\hat{u}$ are OOD? [MJ: 50%]
And the density estimator is probably too smooth. It would be good to show the OOD points used to train $\hat{u}$.[MJ: Done]

After Tuning KDE:
![](https://i.imgur.com/D5YGsPf.png)
 --><p><strong>Newer Results</strong>: Previously due to an error the network was being
trained on the density of the estimator, but at test time the log
density was being used, leading to wrong estimates.</p>
<p><strong>Without OOD</strong>:</p>
<p><img alt="image22" src="https://i.imgur.com/MS6ZoGs.png" /> <img alt="image23" src="https://i.imgur.com/cxO9pB5.png" /> <img alt="image24" src="https://i.imgur.com/fFTfV53.png" /> <img alt="image25" src="https://i.imgur.com/2crB8ay.png" /> <img alt="image26" src="https://i.imgur.com/xKMhZqB.png" /></p>
<p><strong>With OOD</strong> <img alt="image27" src="https://i.imgur.com/x2FPaVI.png" /> <img alt="image28" src="https://i.imgur.com/RMN8Wyk.png" /> <img alt="image29" src="https://i.imgur.com/GLDODhb.png" /> <img alt="image30" src="https://i.imgur.com/0qRdwP8.png" /> <img alt="image31" src="https://i.imgur.com/tHqmLzm.png" /></p>
<p><strong>Switch to estimating e instead of u</strong> Also randomly sample points as
N/2 OOD points <img alt="image32" src="https://i.imgur.com/Kve4Nvr.png" /> <img alt="image33" src="https://i.imgur.com/LBQT05k.png" /> <img alt="image34" src="https://i.imgur.com/4bfxcP0.png" /></p>
<p>For comparision Squared Error <img alt="image35" src="https://i.imgur.com/hRE7cgG.png" /></p>
<p>MSE: Our Approach: 0.016015647 MC Dropout: 0.22903986 Single Model
Uncertainties: 0.1424897</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/pdf/1811.00908.pdf">Single-Model Uncertainties for Deep Learning [NeurIPS
2019]</a> on the same data
<img alt="image36" src="https://i.imgur.com/zASFhP6.png" /></p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1506.02142.pdf">MC Dropout</a> <img alt="image37" src="https://i.imgur.com/TDsHhj2.png" /></p></li>
<li><p>Gaussian Proccess <img alt="image38" src="https://i.imgur.com/MpZ0ox4.png" /></p></li>
</ul>
<p>Comments: * <span class="math notranslate nohighlight">\(f\)</span> is not trained on the uniformly sampled examples
so they are OOD, thus we can measure the true out sample error for these
points. * There is L2 regularization on <span class="math notranslate nohighlight">\(\hat{a}\)</span>, otherwise it
sometimes just collpases to zero. * As of now, the OOD samples are not
added to the retraining for <span class="math notranslate nohighlight">\(\hat{u}\)</span> step, which would explain
why there is not much difference when they are added. Next thing to try
is add these points in retraining for <span class="math notranslate nohighlight">\(\hat{u}\)</span>. * Another thing
to note is the networks are extremely sensitive to the learning rate,
and require lot of careful tuning.</p>
<p><strong>Relevant Papers to look at</strong> [<em>Very similar, uses loss prediction for
active learning</em>] <a class="reference external" href="https://arxiv.org/pdf/1905.03677.pdf">https://arxiv.org/pdf/1905.03677.pdf</a></p>
<p><a class="reference external" href="http://www.columbia.edu/~jwp2128/Papers/LiuPaisleyetal2019.pdf">http://www.columbia.edu/~jwp2128/Papers/LiuPaisleyetal2019.pdf</a>
<a class="reference external" href="https://arxiv.org/pdf/1506.02142.pdf">https://arxiv.org/pdf/1506.02142.pdf</a>
<a class="reference external" href="https://arxiv.org/pdf/1811.00908.pdf">https://arxiv.org/pdf/1811.00908.pdf</a></p>
<p><strong>Things to try</strong>: * log density to be used as feature * Add OOD
samples to retraining step for <span class="math notranslate nohighlight">\(\hat{u}\)</span> [Done] * Maybe use other
models to estimate density, instead of KDE. * YB: try larger kernel
(use cross-validation) for KDE. [Done] * YB: show the OOD samples on
the figu re [Done] * YB: make sure <span class="math notranslate nohighlight">\(\hat{u}\)</span> is well trained
(which requires keeping the OOD samples in its training set, indeed).
Maybe play with learning rate schedule or adaptive learning rate method.
* YB: compare with other baselines (in particular MC-dropout). [GP
Partly Done] * YB: Active learning not a fixed set points, but on
environment? (Dropout paper) * YB: Using our uncertainty estimator for
active learning</p>
</div>
</div>
</div>
<hr class="docutils" />
<div class="section" id="sumana">
<h2>Sumana<a class="headerlink" href="#sumana" title="Permalink to this headline">¶</a></h2>
</div>
<hr class="docutils" />
<div class="section" id="howard">
<h2>Howard<a class="headerlink" href="#howard" title="Permalink to this headline">¶</a></h2>
</div>
<hr class="docutils" />
<div class="section" id="victor">
<h2>Victor<a class="headerlink" href="#victor" title="Permalink to this headline">¶</a></h2>
<div class="section" id="reward-predictor">
<h3>Reward Predictor<a class="headerlink" href="#reward-predictor" title="Permalink to this headline">¶</a></h3>
<p>Instead of using full on docking to approximate how well our candidate
molecule binds to a target, it would be incredibly useful to have a
cheap approximator that could accurately predict binding energy.</p>
<p>Note: one metric used many times over the course of these experiments is
“regret”. Over the course of feeding batches of data (batches of mols in
this case), there exists some ranking of the all the data seen so far by
some metric; for this experiment its the energy of the molecule and we
rank low to high. Whenever we feed data to the model, it produces a
prediction for all the mols in the batch, and from these predictions,
and all those seen before, we construct a second ranking where we rank
from lowest to highest; this time, the lowest being the real energy from
the mol corresponding to the lowest predicted energy. The difference
between these two rankings is regret, and we consider the difference of
medians, MAE, and MSE regret.</p>
</div>
<div class="section" id="backpropagated-loss">
<h3>Backpropagated Loss<a class="headerlink" href="#backpropagated-loss" title="Permalink to this headline">¶</a></h3>
<p>One thing to try first was in general, do we see a perfomance difference
between backpropagating L2-loss vs L1-loss? <img alt="image39" src="https://i.imgur.com/JwgTaU5.png" /> <img alt="image40" src="https://i.imgur.com/TlnMSLb.png" /></p>
<p>Preliminarily it seems that for the task of learning our MPNN it doesn’t
seem to make a noticable difference; however, for the purpose of fine
tuning the reward predictor, we still try backpropagating L2 and L1 loss
for each sampler scheme.</p>
</div>
<div class="section" id="non-uniform-sampling-of-molecules-for-training">
<h3>Non-uniform sampling of molecules for training<a class="headerlink" href="#non-uniform-sampling-of-molecules-for-training" title="Permalink to this headline">¶</a></h3>
<p>In the attempt to focus on getting good accuracy for molecules with
lower energy, one attempt that we studied fairly extensively has been
how oversampling lower energy molecules affects preformance in terms of
regret.</p>
<p>To do this we changed our dataloaders from just iterating through the
dataset without replacement to sampling with replacement, where each
molecule was sampled with probability
<span class="math notranslate nohighlight">\(p = (\frac{\text{mol energy}}{\text{sum of mol energies}})^n + \epsilon\)</span>
where n is a hyper-parameter determining how aggresively we oversample
high energy molecules.</p>
</div>
<div class="section" id="id2">
<h3>Results<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p>In order to show the runs clearly, here is first a high level analysis
of the regret curves for top 15 median difference for regret and top 50
median difference for regret.</p>
<div class="figure align-default">
<img alt="" src="https://i.imgur.com/HLMul4g.png" />
</div>
<p>It seems from many different sampling schemes, with powers of n ranging
between 1 to 5, and a run with power 20, there doesn’t seem to be a
difference in the prediction of the 15th or 50th ranked energy on our
validation set.</p>
<p>Note: It was brought up at the last meeting that maybe we should be
measuring our preformance on the training set; however, since we are
expecting this model to transfer to any manner of molecule we might see
in our search, we think this might be more representative of the models
ability to distinguish high energy molecules.</p>
</div>
<div class="section" id="training-without-low-energy-tail">
<h3>Training without low energy tail<a class="headerlink" href="#training-without-low-energy-tail" title="Permalink to this headline">¶</a></h3>
<p>Our training and validation sets are composed of uniformely distributed
(acording to their energy) molecules, except for a small “tail” towards
the low end of the energy distribution where there is a much small group
of very low energy molecules.</p>
<p>Because in reality we hope to see molecules with even lower energy than
what we have in our training/validation sets, we wanted to investigate
how well our model would generalize to the validation set with higher
energies included, if we didn’t train with the tail included. Here are
the results:</p>
<div class="figure align-default">
<img alt="" src="https://i.imgur.com/sVEkzmR.png" />
</div>
<p>The sit of curves that converge in the top group were all trained with
no tail, and those below with. Its clear that our model overfits to the
training data, and doesn’t generalize well to higher-energy molecules.
This is an issue that is known with our RL experiments but is reinforced
here.</p>
<p><strong>Things to try out</strong> * Try different MPNN architecture, especially
directed message passing neural networks (dimenet)</p>
</div>
</div>
<hr class="docutils" />
<div class="section" id="joanna">
<h2>Joanna<a class="headerlink" href="#joanna" title="Permalink to this headline">¶</a></h2>
<p>We begin this section by describing our task and the dataset we use. We
describe our base architecture before then going on to describe two ways
we convert it into a Bayesian model to enable the generation of
uncertainties. We then detail how we evaluate our models, before
finishing by describing our acquisition function (i.e. how we use our
model to decide on which molecules to obtain the data for next). The
next section goes onto evaluate these models.</p>
<p>Let <span class="math notranslate nohighlight">\(G\)</span> be the space of molecule graphs. Our base model is a
Message Passing Neural Network (MPNN) <span class="math notranslate nohighlight">\(f: G \rightarrow R^{64}\)</span>
and 2 layers MLP <span class="math notranslate nohighlight">\(g: R^{64} \rightarrow R\)</span> with dropout on the
last layer only, as default. Let <span class="math notranslate nohighlight">\(\theta=\{\theta_f, \theta_g\}\)</span>.
The objective function is <span class="math notranslate nohighlight">\(min_\theta\sum_{x,y}||g(f(x)) - y||^2\)</span>
where <span class="math notranslate nohighlight">\(x \in G\)</span> and <span class="math notranslate nohighlight">\(y\)</span> is the ground truth. The datasets we
use are Zinc15_2k and Zinc15_260k. We will denote the number of
datapoints, indicated by the number after the underscore in the dataset
name (i.e. 2k in ‘Zinc15_2k’), by <span class="math notranslate nohighlight">\(n\)</span> where
<span class="math notranslate nohighlight">\(n \in \{2000, 260000\}\)</span>.</p>
<p>There are two ways to convert our base model to turn them into Bayesian
models to allow the calculation of uncertainties. Each will give a
posterior predictive distribution based on mean and variance to compute
uncertainty and we can use log-likelihood to evaluate how good these
values are. These are as follows: 1. MPNN + MC dropout: Use Gal and
Ghahramani [2016] to compute mean and variance. The log-likelihood to
evaluate uncertaintainties is</p>
<div class="math notranslate nohighlight">
\[\log p\left(\mathbf{y}^{*} \mid \mathbf{x}^{*}, \mathbf{X}, \mathbf{Y}\right) \approx \operatorname{logsumexp}\left(-\frac{1}{2} \tau\left\|\mathbf{y}-\widehat{\mathbf{y}}_{t}\right\|^{2}\right)-\log T-\frac{1}{2} \log 2 \pi-\frac{1}{2} \log \tau^{-1}\]</div>
<p>where <span class="math notranslate nohighlight">\(T\)</span> is the number of forward passes performed, and
<span class="math notranslate nohighlight">\(\tau\)</span>, the precision, is set from the hyperparameter as
<span class="math notranslate nohighlight">\(\tau=\frac{p l^{2}}{2 N \lambda}\)</span> where <span class="math notranslate nohighlight">\(p\)</span> is <span class="math notranslate nohighlight">\(1 -\)</span>
dropout probability, <span class="math notranslate nohighlight">\(l\)</span> is the prior length-scale, <span class="math notranslate nohighlight">\(N\)</span> is
the length of training set and <span class="math notranslate nohighlight">\(\lambda\)</span> is the weight decay
hyperparameter. In our case, we set the values T = 20, p = 0.9 and
<span class="math notranslate nohighlight">\(\lambda\)</span> = 1e-8 as default, and later do grid search to find the
optimal value for <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
<ol class="arabic" start="2">
<li><p>Bayesian: Use Bayesian ridge regression to compute mean and variance.
After the training is done, we get the embeddings
<span class="math notranslate nohighlight">\(E_{64 \times n}\)</span> before the last layer of the MLP and fit a
Bayesian ridge regression <span class="math notranslate nohighlight">\(h: E \rightarrow y\)</span> without using MC
dropout. The log-likelihood to evaluate uncertaintainties is</p>
<div class="math notranslate nohighlight">
\[l(\hat\mu, \hat\sigma^{2}; y)=-\frac{n}{2} \ln (2 \pi\hat\sigma^{2})-\frac{1}{2 \hat\sigma^{2}} \sum_{j=1}^{n}\left(y_{j}-\hat\mu\right)^{2}.\]</div>
</li>
</ol>
<div class="line-block">
<div class="line">As well as log likelihood we also consider the following metrics: -</div>
</div>
<p>MAE (mean absolute error)
| - MSE (mean squared error) - Aq_Top15: Median of sorted top 15 ground
truths from acquirer
| - Aq_Top50: Median of sorted top 50 ground truths from acquirer
| - Top 15 regret: Let <span class="math notranslate nohighlight">\(y^{gt} = \{y_{g(1)},..,y_{g(N)}\}\)</span> be the
top N energies ranked accordin to their ground truth values and and
<span class="math notranslate nohighlight">\(y^{model} = \{y_{m(1)},..,y_{m(N)}\}\)</span> be the top N energies
ranked according to the predicted energy scores from the model. top 15
regret =
<span class="math notranslate nohighlight">\(\text{median}\{y_{g(1)},..,y_{g(15)}\} - \text{median}\{y_{m(1)},..,y_{m(15)}\}\)</span>.
- Top 50 regret: similarly, top 50 regret =
<span class="math notranslate nohighlight">\(\text{median}\{y_{g(1)},..,y_{g(50)}\} - \text{median}\{y_{m(1)},..,y_{m(50)}\}\)</span>.</p>
<p>Then we added an Upper Confidence Bound (UCB) acquisition function to
acquire data. Suppose <span class="math notranslate nohighlight">\(D_t\)</span> is the dataset for training and
<span class="math notranslate nohighlight">\(D_r\)</span> is the remaining dataset. Everytime we train a Bayesian
model on <span class="math notranslate nohighlight">\(D_t\)</span> and apply it to <span class="math notranslate nohighlight">\(D_r\)</span> to get the above
metrics. We use the mean <span class="math notranslate nohighlight">\(\hat\mu\)</span> and variance
<span class="math notranslate nohighlight">\(\hat\sigma^{2}\)</span> on <span class="math notranslate nohighlight">\(D_r\)</span> from posterior predictive
distribution given by the Bayesian model to compute UCB scores such that
<span class="math notranslate nohighlight">\(\text{UCB scores} = \hat\mu(x) + \kappa \hat\sigma^{2}(x)\)</span>. We
select points from <span class="math notranslate nohighlight">\(D_r\)</span> based on the UCB scores and add them to
<span class="math notranslate nohighlight">\(D_t\)</span> for training. Repeat this process.</p>
<p>The results of Bayesian ridge regression on the embeddings are shown in
Table 1. It is trained over 61 epochs on 2k dataset and 20 epochs on
260k dataset, and the metrics in Table 1 are recorded at the end of
testing. While the MSE of the test loss shows it overfits a bit, the
260k performs better than 2k according to the MSE, log-likelihood, top
15 and 50 regret on the test set.</p>
<p>Table 1. Bayesian models on the embeddings using 2k and 260k datasets
without using UCB</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 13%" />
<col style="width: 9%" />
<col style="width: 10%" />
<col style="width: 29%" />
<col style="width: 20%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Dataset</p></th>
<th class="head"><p>MAE</p></th>
<th class="head"><p>MSE</p></th>
<th class="head"><p>MPNN+Bayesian log-lik</p></th>
<th class="head"><p>top 15 regret</p></th>
<th class="head"><p>top 50 regret</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>2k</p></td>
<td><p>2.43</p></td>
<td><p>10.37</p></td>
<td><p>-0.67</p></td>
<td><p>2.35</p></td>
<td><p>2.13</p></td>
</tr>
<tr class="row-odd"><td><p>260k</p></td>
<td><p>1.38</p></td>
<td><p>3.11</p></td>
<td><p>-0.12</p></td>
<td><p>1.77</p></td>
<td><p>0.67</p></td>
</tr>
</tbody>
</table>
<p>For the MC-Dropout model variant, we want to find the regularization
term coefficient <span class="math notranslate nohighlight">\(\lambda\)</span> which gives the highest log-likelihood.
Consider MPNN with three dropout cases: (1) dropout on the last layer
only (2) dropout on the weights of the entire network, including the
last layer (3) dropout on input and output data, weights, and the last
layer. Each of these different case will add varying levels of
uncertainty.</p>
<p>For each case, we did grid search on ten <span class="math notranslate nohighlight">\(\lambda\)</span> values which
were equally partitioned from the range of 1e-4 to 1e-12; they are
{1e-4, 1.29e-5, 1.67e-6, 2.15e-7, 2.78e-8, 3.59e-9, 4.61e-10, 5.99e-11,
7.74e-12, 1e-12}. We included the best result of each metric after 61
epochs on 2k dataset and after 20 epochs on 260k dataset as well as the
corresponding <span class="math notranslate nohighlight">\(\lambda\)</span> in Table 2. In the table, the optimal
lambda which gives the best log-likelihood is consistent on both
datasets where <span class="math notranslate nohighlight">\(\lambda^*\)</span>=1e-9 on the 2k dataset and
<span class="math notranslate nohighlight">\(\lambda^*\)</span> = 1e-11 on the 260k dataset. Moreover, these
<span class="math notranslate nohighlight">\(\lambda^*\)</span> values give the global maximum on [1e-4, 1e-12]; the
likelihood increases as <span class="math notranslate nohighlight">\(\lambda\)</span> decreases until the value
reaches the maxima at each <span class="math notranslate nohighlight">\(\lambda^*\)</span>, after which, the
likelihood decreases. Then we did grid search within a contracted range
from 1e-8 to 1e-12 again. We got a consistent conclusion and a more
precise <span class="math notranslate nohighlight">\(\lambda\)</span> that the dropout case (2) gives the highest
log-likelihood on the 2k dataset with $^*$ = 6.16e-9, and (2)(3) both
give the highest log-likelihood with <span class="math notranslate nohighlight">\(\lambda^*\)</span> = 5.99e-11 on the
260k dataset.</p>
<p>Comparing Table 2 with Table 1, we can see that on the 2k dataset, MC
dropout gives slightly higher log-likelihood in the dropout case (2); on
the 260k dataset, MC dropout gives much higher log-likelihood than
Bayesian model in all three dropout cases.</p>
<p>Table 2. Grid search on <span class="math notranslate nohighlight">\(\lambda\)</span> ranges from 1e-4 to 1e-12 with
three different dropout cases</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 5%" />
<col style="width: 13%" />
<col style="width: 14%" />
<col style="width: 15%" />
<col style="width: 15%" />
<col style="width: 22%" />
<col style="width: 16%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Dataset</p></th>
<th class="head"><p>Dropout</p></th>
<th class="head"><p>MAE</p></th>
<th class="head"><p>MSE</p></th>
<th class="head"><p><strong>log-lik</strong></p></th>
<th class="head"><p>top 15 regret</p></th>
<th class="head"><p>top 50 regret</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>2k</p></td>
<td><p>Last layer only</p></td>
<td><p>2.10 (<span class="math notranslate nohighlight">\(\lambda\)</span>=e-9)</p></td>
<td><p>8.9 (<span class="math notranslate nohighlight">\(\lambda\)</span>=e-9)</p></td>
<td><p>-0.73 (<span class="math notranslate nohighlight">\(\lambda\)</span>=e-9)</p></td>
<td><p>0 (<span class="math notranslate nohighlight">\(\lambda\)</span>=e-9)</p></td>
<td><p>7e-3 (<span class="math notranslate nohighlight">\(\lambda\)</span>=e-9)</p></td>
</tr>
<tr class="row-odd"><td><p>2k</p></td>
<td><p>Last layer + weights</p></td>
<td><p>2.25 (<span class="math notranslate nohighlight">\(\lambda\)</span>=e-5)</p></td>
<td><p>8.9 (<span class="math notranslate nohighlight">\(\lambda\)</span>=e-12)</p></td>
<td><p>-0.65 (<span class="math notranslate nohighlight">\(\lambda\)</span>=e-9)</p></td>
<td><p>0 (<span class="math notranslate nohighlight">\(\lambda\)</span>=e-5,e-6,e-7,e-8,e-11,e-12)</p></td>
<td><p>0.1 (<span class="math notranslate nohighlight">\(\lambda\)</span> =e-7, e-11)</p></td>
</tr>
<tr class="row-even"><td><p>2k</p></td>
<td><p>Last layer + weights + data</p></td>
<td><p>2.40 (<span class="math notranslate nohighlight">\(\lambda\)</span>=e-6)</p></td>
<td><p>9.99 (<span class="math notranslate nohighlight">\(\lambda\)</span>=e-10)</p></td>
<td><p>-0.69 (<span class="math notranslate nohighlight">\(\lambda\)</span>=e-9)</p></td>
<td><p>0 (<span class="math notranslate nohighlight">\(\lambda\)</span>=e-8,e-9,e-11,e-12)</p></td>
<td><p>0.11 (<span class="math notranslate nohighlight">\(\lambda\)</span>=e-12)</p></td>
</tr>
<tr class="row-odd"><td><p>260k</p></td>
<td><p>Last layer only</p></td>
<td><p>1.36 (<span class="math notranslate nohighlight">\(\lambda\)</span>=e-6)</p></td>
<td><p>3.002 (<span class="math notranslate nohighlight">\(\lambda\)</span>=e-6)</p></td>
<td><p>-0.04 (<span class="math notranslate nohighlight">\(\lambda\)</span>=e-11)</p></td>
<td><p>1.12 (<span class="math notranslate nohighlight">\(\lambda\)</span>=e-9)</p></td>
<td><p>1.42 (<span class="math notranslate nohighlight">\(\lambda\)</span>=e-11)</p></td>
</tr>
<tr class="row-even"><td><p>260k</p></td>
<td><p>Last layer + weights</p></td>
<td><p>1.34 (<span class="math notranslate nohighlight">\(\lambda\)</span>=e-11)</p></td>
<td><p>2.92(<span class="math notranslate nohighlight">\(\lambda\)</span>=e-10)</p></td>
<td><p>-0.01 (<span class="math notranslate nohighlight">\(\lambda\)</span>=e-11)</p></td>
<td><p>1.44 (<span class="math notranslate nohighlight">\(\lambda\)</span>=e-12)</p></td>
<td><p>0.92 (<span class="math notranslate nohighlight">\(\lambda\)</span>=e-11)</p></td>
</tr>
<tr class="row-odd"><td><p>260k</p></td>
<td><p>Last layer + weights + data</p></td>
<td><p>1.34 (<span class="math notranslate nohighlight">\(\lambda\)</span>=e-7)</p></td>
<td><p>2.90 (<span class="math notranslate nohighlight">\(\lambda\)</span>=e-7)</p></td>
<td><p>-0.01 (<span class="math notranslate nohighlight">\(\lambda\)</span>=e-11)</p></td>
<td><p>1.38 (<span class="math notranslate nohighlight">\(\lambda\)</span>=e-6)</p></td>
<td><p>1.23 (<span class="math notranslate nohighlight">\(\lambda\)</span>=e-5,e-12)</p></td>
</tr>
</tbody>
</table>
<p>We added a UCB acquisition function to acquire data. First we acquired
data using UCB but with a large noise (<span class="math notranslate nohighlight">\(\epsilon = 1000\)</span>) on the
large and small datasets and let it be the random baseline. We put the
results of UCT random baseline, Bayesian model, MPNN with default
<span class="math notranslate nohighlight">\(\lambda\)</span> together for each experiment on 260k. The top 15 and top
50 regrets are shown in Figure 1. Note that as epochs increase, the
number of datapoints in <span class="math notranslate nohighlight">\(|D_t|\)</span> increases and the number of
datapoints in <span class="math notranslate nohighlight">\(|D_r|\)</span> decreases. Although we only ran the random
acquisition for 20 epochs, we can still see at the same point in time
using a Bayesian or MC dropout model would have given better results at
20 epochs.</p>
<p><img alt="Top 15 and Top 50 regret of three different dropout cases on 260k dataset" src="https://i.imgur.com/y15MItk.png" /> Figure 1: Top 15 and Top 50 regret of three different dropout
cases on 260k dataset</p>
<p>To explore the exploration/exploitation tradeoff, we did grid search on
<span class="math notranslate nohighlight">\(\kappa \in \{0.01, 0.1, 1, 10\}\)</span> over 20 epochs on the 260k
dataset twice and got inconsistent results, the inconsistency can be
ignored because the number of epochs at which we reach covergence is
very similar between experiments (Figure 2). In Figure 3, we compare the
results with random baseline together. Results also shows that all
kappas works similar to each other and random baseline performs much
worse. <img alt="Results from running grid search of kappa on 260k dataset twice" src="https://i.imgur.com/iGK1oTX.png" />
Figure 2: Results from running grid search of kappa on 260k dataset
twice</p>
<p><img alt="Results from random baseline and grid search of kappa on 260k dataset" src="https://i.imgur.com/4VChU0Q.png" />
Figure 3: Results from random baseline and grid search of kappa on 260k
dataset</p>
</div>
<hr class="docutils" />
<div class="section" id="samin">
<h2>Samin<a class="headerlink" href="#samin" title="Permalink to this headline">¶</a></h2>
<div class="section" id="graph-bottleneck">
<h3>Graph_bottleneck<a class="headerlink" href="#graph-bottleneck" title="Permalink to this headline">¶</a></h3>
<p><strong>MDP</strong></p>
<p>At timestep <span class="math notranslate nohighlight">\(t\)</span>, state <span class="math notranslate nohighlight">\(s_t \in S\)</span> is attribute (feature) of
the current node, action <span class="math notranslate nohighlight">\(a_t \in A\)</span> is choosing the next node to
pass the message. Reward for passing message through the nodes is
computed using <span class="math notranslate nohighlight">\(v_\phi(s,a)\)</span>. The objective function
<span class="math notranslate nohighlight">\(v_\phi\)</span> want to maximize: <span class="math notranslate nohighlight">\(J(\phi) =\)</span></p>
<p><strong>REINFORCE in practice</strong></p>
<p>” sample <span class="math notranslate nohighlight">\({ \{ s_t,a_t ... s_T\} } \sim \pi_{\theta}(.)\)</span> Update
value approximation <span class="math notranslate nohighlight">\(v_\phi\)</span> using
<span class="math notranslate nohighlight">\(\phi \leftarrow \phi + \nabla_\phi J(\phi)\)</span> (need to define
<span class="math notranslate nohighlight">\(J(\phi)\)</span> ) for each timestep <span class="math notranslate nohighlight">\(t=\{0,1..T-1\}\)</span>:
—-<span class="math notranslate nohighlight">\(G_t = \sum_{k=t}^T\gamma^k r(s_k,a_k)\)</span>
<span class="math notranslate nohighlight">\(\min \nabla_\theta J(\theta) = -\sum_t G_t\nabla_\theta\log\pi_\theta(a_t|s_t)\)</span>
(here, using <span class="math notranslate nohighlight">\(v_\phi(s,a)\)</span> instead of<span class="math notranslate nohighlight">\(G_t\)</span>) “</p>
<p><strong>CEM in practice</strong></p>
<p>” sample <span class="math notranslate nohighlight">\(\{\tau_i\}_{i=1}^N \sim \pi_\theta(a_t|s_t)\)</span> where
<span class="math notranslate nohighlight">\({ \tau = \{ s_t,a_t ... s_T\} }\)</span> sort best performing
<span class="math notranslate nohighlight">\(\tau\)</span> using <span class="math notranslate nohighlight">\(v_\phi\)</span> Update value approximation
<span class="math notranslate nohighlight">\(v_\phi\)</span> using <span class="math notranslate nohighlight">\(\phi \leftarrow \phi + \nabla_\phi J(\phi)\)</span>
(need to define <span class="math notranslate nohighlight">\(J(\phi)\)</span> ) for each <span class="math notranslate nohighlight">\(\tau\)</span> —-for each
timestep <span class="math notranslate nohighlight">\(t=\{0,1..T-1\}\)</span>:
——–<span class="math notranslate nohighlight">\(G_t = \sum_{k=t}^T\gamma^k r(s_k,a_k)\)</span>
<span class="math notranslate nohighlight">\(\min \nabla_\theta J(\theta) = -\sum_\tau\sum_t G_t\nabla_\theta\log\pi_\theta(a_t|s_t)\)</span>
(here, using <span class="math notranslate nohighlight">\(v_\phi(s,a)\)</span> instead of<span class="math notranslate nohighlight">\(G_t\)</span>) “</p>
<div class="figure align-default">
<img alt="" src="https://imgur.com/ggLH7UD.png" />
</div>
</div>
<div class="section" id="cross-entropy-method">
<h3>Cross entropy method<a class="headerlink" href="#cross-entropy-method" title="Permalink to this headline">¶</a></h3>
<p><strong>Psuedo-code</strong>: (<a class="reference external" href="https://arxiv.org/pdf/1909.12830.pdf">source</a>)</p>
<p>” sample <span class="math notranslate nohighlight">\({ \{ x_{i,t} \} }_{i=1}^N \sim g_{\phi_t}(.)\)</span> evaluate
<span class="math notranslate nohighlight">\(v_{t,i} := f_\theta(x_{t,i})\)</span> sort top samples refit
<span class="math notranslate nohighlight">\(g_\phi\)</span> with top samples:
<span class="math notranslate nohighlight">\(\phi_{t+1}=argmax_\phi \sum_i \mathbf{1}\{ P \} \log g_\phi(x_{i,t})\)</span></p>
<p>“</p>
<p>where <span class="math notranslate nohighlight">\(P\)</span> is a condition, if true consider the sample <span class="math notranslate nohighlight">\(i\)</span>,
here the condition is putting a threshold on the reward. Sample
<span class="math notranslate nohighlight">\((s,a)\)</span> pairs if <span class="math notranslate nohighlight">\(r(s,a)\geq threshold\)</span>.</p>
<p><strong>Code</strong>: * <a class="reference external" href="https://github.com/Neo-47/CE-methods-on-gym-environments/blob/master/cartpole.py#L100">CEM implementation on
CartPole-v0</a>
* NOTE:
(<a class="reference external" href="https://gist.github.com/domluna/022e73fd5128b05bdd96d118b5131631">source</a>)
Preliminary investigation showed that applicability of CE to RL problems
is restricted severly by the phenomenon that the distribution
concentrates to a single point too fast. To prevent this issue, noise is
added to the previous stddev/variance update calculation.</p>
<p><strong>NOTE</strong>: * It’s a non-parametric approach (not gradient based like
most policy search) * Here’s new paper which proposes “Differentiable
CEM” <a class="reference external" href="https://arxiv.org/pdf/1909.12830.pdf">https://arxiv.org/pdf/1909.12830.pdf</a></p>
<p><strong>Reads</strong>: * <a class="reference external" href="https://photos.google.com/share/AF1QipODcxqYaw37s5d39PCZGX85HY4BsX9vrn8KnT_Q2KJ8L7awCKCPhCZybX8iYtc8XA?key=NGVSNHFzSGFGU0pabjBzVWZJdThZUEN5SVc5UEp3">Reference shared by
pierre-luc</a>
* <a class="reference external" href="https://www.aaai.org/Papers/ICML/2003/ICML03-068.pdf">The Cross Entropy method for Fast Policy
Search</a> (<strong>need
to check</strong>) * Proposes an smoothing update:
<span class="math notranslate nohighlight">\(\phi_{t+1}=\alpha\phi_{t+1}+(1-\alpha)\phi_t\)</span> * <a class="reference external" href="https://people.smp.uq.edu.au/DirkKroese/ps/aortut.pdf">A Tutorial on
the Cross-Entropy
Method</a></p>
<p><strong>Good Repo</strong> *
<a class="reference external" href="https://github.com/eleurent/rl-agents#cem-cross-entropy-method">https://github.com/eleurent/rl-agents#cem-cross-entropy-method</a> (<strong>need
to check</strong>)</p>
<p><strong>TODO</strong> * Compare CEM with REINFORCE * CEM + Smoothing update
(<a class="reference external" href="https://www.aaai.org/Papers/ICML/2003/ICML03-068.pdf">source</a>) +
adding noise
(<a class="reference external" href="https://gist.github.com/domluna/022e73fd5128b05bdd96d118b5131631">source</a>)
+ regularizer</p>
</div>
</div>
<div class="section" id="summary-of-meetings">
<h2>Summary of meetings<a class="headerlink" href="#summary-of-meetings" title="Permalink to this headline">¶</a></h2>
<div class="section" id="august-26">
<h3>August 26<a class="headerlink" href="#august-26" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Moksh, restart buffer: problems with the restart buffer, using Morgan
Fingerprints to add the buffer or reject, with buffer pruning after a
certain size. Problem: no exploration -&gt; start the episode 50% of the
time from the buffer, 50% from a random walk, this improves things
quite a bit. Could explore parameter <span class="math notranslate nohighlight">\(p\)</span>, currently 50% in
conjuction with similarity threshold.</p>
<ul>
<li><p>Yoshua: molecules are pruned based on reward, but inserted based
on similarity, perhaps we should also use the reward/energy to
accept/reject molecules. Could also keep track of a mean/variance
estimate of Value. Compare stored gaussians with incoming
molecules.</p></li>
<li><p>Sanity check experiment on threshold: high thresholds is
equivalent to adding blindly with p=50%</p></li>
<li><p>Why does always adding to the buffer hurts? This probably hurts
exploration, we’d end up seeing molecules twice as much as
necessary.</p></li>
<li><p>On initial state distribution: Pierre-Luc this could be a problem,
Yoshua: well the buffer could be a statistic of the “search
state”, Doina: this might not even be an RL problem so it may not
matter. You can start a search from anywhere.</p></li>
<li><p>Yoshua: we should aim to understand this system as a search and
the buffer as the state. Pierre-Luc: the “RL” algorithms come with
certain assumptions, if we’re not careful about the assumptions
we’re making. If they don’t then it shouldn’t be too surprising.</p></li>
<li><p>Yoshua could sample from softmax instead of uniformly. Could also
use expected improvement, Thompson sampling.</p></li>
<li><p>Should the value be a max? Then the uncertainty model will be
different. Emmanuel: but the tail of the max is very noisy (from
the fat tail) so this may not be very useful. In experiments not
very useful after some number of molecules (check with Bogdan?)</p></li>
</ul>
</li>
<li><p>Moksh, uncertainty prediction:</p>
<ul>
<li><p>Why is the predicted uncertainty so smooth? Not enought
capacity/data? Doina: test this hypothesis with more points.</p></li>
<li><p>There’s a problem with measure of true epistemic uncertainty. Need
to take difference between
<span class="math notranslate nohighlight">\((f_\theta(x) - \mathbb{E}_\eta[f(x;\eta)])^2\)</span> (<span class="math notranslate nohighlight">\(\eta\)</span>
the noise)</p></li>
<li><p>MC Dropout almost done</p></li>
<li><p>When this works for a stationary setting, then we can move to
non-stationary, active learning.</p></li>
</ul>
</li>
<li><p>Victor: 7m molecule dataset, sample proportional to the energy, does
it get better. Initially this did not outperform uniform sampling:</p>
<ul>
<li><p>Limiting factor in RL: MPNN is not good on high-reward molecules.</p></li>
<li><p>This may be fixed by changing the training objective, e.g.
selecting for regret, true ranking of molecules.</p></li>
<li><p>Oversampling methods do not seem to improve regret.</p></li>
<li><p>Now, checking effect of changing from uniform to gaussian
sampling, how does top-15 top-50 regret changes?</p></li>
<li><p>Doina: we don’t only care about the ranking, at the high end the
energies become extreme and we care about the values. An error of
.2 is not the same in the middle than in the high end. Literature
on search engines might be interesting.</p></li>
<li><p>We could train a bunch of MPNN on each bin of energy. Then you
need an MPNN to say which bin to send the molecule to.</p></li>
</ul>
</li>
<li><p>How should we train the reward neural net? Ideally, with respect to
the distribution it will see when it is used. E.g. the kind of visits
a search/RL algorithm would see. Uniform might be a proxy but not the
best.</p></li>
</ul>
</div>
<div class="section" id="august-19-8-30">
<h3>August 19 8:30<a class="headerlink" href="#august-19-8-30" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Maksym will focus on fixing a bug which should improve the reward
learning (not much details)</p></li>
<li><p>Moksh: he verfied if he can recover the PPO performance when he sets
alpha to 0, also working on a bug</p></li>
<li><p>Moksh: improve plots and will add legends, some issues with reward</p></li>
<li><p>Moksh: instability in learning, maybe due to LR</p></li>
<li><p>Moksh: try shorter runs, models are flattened, increase number of
actors</p></li>
<li><p>Moksh: evaluate using docking once in a while</p></li>
<li><p>Use same metrics as Emmanuel</p></li>
<li><p>Moksh: in uncertainty learning, make sure you have samples in outside
regions, use estimate of density as feature, i.e. MC dropout, picking
a random molecule can help sampling OOD, another idea is to use GT as
benchmark…</p></li>
<li><p>Emmanuel: distributed alphazero</p></li>
<li><p>Organization of project: Adding a spreadsheet for ideas, extend the
hackmd for whole project</p></li>
</ul>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="generated/LambdaZero.chem.html" class="btn btn-neutral float-right" title="LambdaZero.chem package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="index.html" class="btn btn-neutral float-left" title="Welcome to LambdaZero documentation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, LambdaZero Collaboration

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>